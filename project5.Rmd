---
title: 'writeup project: machine learning'
author: "Caxixi"
date: "Tuesday, April 21, 2015"
output: html_document
---

R version 3.1.1
RStudio version  0.98.1056

#introduction#

Data was collected from accelerometers on the belt, forearm, arm, and barbell of 6 participant performing barbell lifts correctly (A) and incorrectly in four different ways (B, C, D, E, see details at: http://groupware.les.inf.puc-rio.br/har, the section on the Weight Lifting Exercise Dataset). We are asked to build a machine learning model that would predict, based on the output from the accelerators, the way the lifts were performed.   

#building the model#

Calling the libraries used:
```{r}
library (caret)
```

Downloading and reading the data used to build the model:
```{r}
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.cvs")
data<-read.csv("training.cvs", header = TRUE, na.strings = c("NA", ""))
```

Breaking the data to training and testing:
```{r}
set.seed(42)
inTrain <- createDataPartition(y=data$classe, p=0.7, list=FALSE)
train_part <- data[inTrain, ]; test_part <- data[-inTrain, ]
dim(train_part); dim(test_part)
```

These are many variables, but turned out many of them are almost all missing data:

```{r checking for NA}
missing_values<- sapply(train_part, function(x) {sum(is.na(x))})
table(missing_values)
```

Those variables are presumably not very helpful, so I opt to ignore them. Creating a data frame without those columns:

```{r dataset with no missing values}
full_cols<-names(missing_values[missing_values==0])
train_part_a<-train_part[, names(data) %in% full_cols]
```                        

In addition, looking at the names of the columns, columns 1 to 7 contain data that are not expected to be related to the outcome, and I opt to remove them as well.
```{r}
train_part_b<-train_part_a[, 8:60]
```

Ideally, one would go through each of those variables, evaluate and perhaps pre-process them. However, this is not very practical with so many variables and my limited mathematical background. The group that gathered this dataset applied a data selection method based on correlation (M. A. Hall, Correlation-based Feature Subset Selection for Machine Learning. PhD thesis, Department of Computer Science, University of Waikato, Hamilton, New Zealand, Apr. 1999). Certainly, numerous variable in this dataset correlate with each other: 
```{r t}
corr<-abs(cor(train_part_b[,-53]))
diag(corr)<-0 # not counting later correlation between a varialbe and itself
high_corr<-sum(corr>0.8)
high_corr
```

There are `r high_corr` instances of high correlation (>0.8) between two variables in the dataset, which means some variables could be ignored or combined to other variables somehow. Indeed the original work made a good prediction relaying on 17 variables only. However, I, personally, am not familiar with that method, and realize that, in principle, even with highly correlated variables, the small fraction of data that are not correlated could be a key to prediction. I therefore opt to use all the 57 variables.

Sampling some of those variables for inspection:
```{r}
par(mfrow=c(2,2))
plot(train_part_b[,1]); plot(train_part_b[,11]); plot(train_part_b[,21]);plot(train_part_b[,30])
```

These variables are not of normal distribution, but kind of grouped in various ways. I'm assuming this is not unique to these arbitrary chosen variables, but rather, reflects the nature of the data accumulated.  Intuitively, this suggest to me that a tree model would be a good choice. To increase the accuracy of the model, I opted to do a random forest model, which create many trees and weigh them all in deciding on the final tree.

```{r}
model = train(classe~., method="rf", data=train_part_b)
```

```{r}
model
summary(model)
```

Looking at the confusion matrix:

```{r}
train_predictions <- predict(model, train_part_b)
confusionMatrix(train_predictions, train_part_b$classe)
```


#checking the accuacy with out-of-sample data#

*NOTE:* It is argued that there is no need for that when using random forest, since the way the model is constructed, third of the data is left out at each split and later run down the tree to evaluate accuracy (see: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr). However, for the sake of exercise, I opt to build the model with a fraction of the data (the train set) and check it on the rest of the data (the test set).

To do so, data set that was selected out as a test set (named "test_part" above) has to be processed the same way the dataset used to build the model (named"train_part"above) was processed.
```{r}
test_part_a<- test_part[, names(data) %in% full_cols]
test_part_b<-test_part_a[, 8:60]
```

Predicting:
```{r}
test_predictions <- predict(model, test_part_b)
confusionMatrix(test_predictions, test_part_b$classe)
```

**Remark on the results:** As a rule of thumb, models predicts better the data that was used to train them than new data. This is because models likely over fit, considering noise and outliers to be as valid as data that more closely represent what we look to predict. In this case, though, I didn't expect a big difference if at all, since the dataset is large and the fraction that was randomly taken aside as a test sample is likely to be very similar to the fraction that was used to build the model. Indeed, the model predicts the test dataset impressively, but not as well as it predicts the data used to generate it: the train dataset is predicted with 100% accuracy, while the 95% confidence interval of the accuracy of predicting the test dataset is entirely below 100%.
